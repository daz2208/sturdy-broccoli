# =============================================================================
# SyncBoard 3.0 Knowledge Bank - Environment Configuration Template
# =============================================================================
# Copy this file to .env and fill in your actual values
# NEVER commit .env to version control!
# =============================================================================

# REQUIRED: Secret key for JWT token signing
# Generate a secure key: openssl rand -hex 32
# Or: python -c "import secrets; print(secrets.token_hex(32))"
SYNCBOARD_SECRET_KEY=your-secret-key-here-change-in-production

# REQUIRED: OpenAI API key for AI features
# Get from: https://platform.openai.com/api-keys
OPENAI_API_KEY=sk-replace-with-your-actual-openai-key

# AI CONCEPT EXTRACTION: Configure how much content to analyze
# CONCEPT_SAMPLE_SIZE: Maximum characters to analyze (default: 6000)
# Higher values = more accurate but slower and more expensive
CONCEPT_SAMPLE_SIZE=6000

# CONCEPT_SAMPLE_METHOD: How to sample long documents
# "smart" = analyze beginning, middle, and end (recommended)
# "truncate" = only analyze first N characters
CONCEPT_SAMPLE_METHOD=smart

# REDIS CACHING: Save 20-40% on API costs by caching extraction results
# ENABLE_CONCEPT_CACHING: Enable/disable caching (default: true)
ENABLE_CONCEPT_CACHING=true

# CONCEPT_CACHE_TTL_DAYS: How long to cache concept extraction results (default: 7 days)
CONCEPT_CACHE_TTL_DAYS=7

# SIMILARITY_CACHE_TTL_DAYS: How long to cache similarity scores (default: 30 days)
SIMILARITY_CACHE_TTL_DAYS=30

# DATABASE (Phase 6): PostgreSQL connection
# Format: postgresql://user:password@host:port/database
# For Docker: postgresql://syncboard:syncboard@db:5432/syncboard
# For local dev: postgresql://syncboard:syncboard@localhost:5432/syncboard
# Fallback SQLite: sqlite:///./syncboard.db
DATABASE_URL=postgresql://syncboard:syncboard@db:5432/syncboard

# SECURITY: Allowed CORS origins (comma-separated)
# Development: Allow localhost on common ports
# Production: Change to your actual domain(s)
SYNCBOARD_ALLOWED_ORIGINS=http://localhost:3000,http://localhost:8000,http://127.0.0.1:3000,http://127.0.0.1:8000

# OPTIONAL: Authentication settings
# 1440 minutes = 24 hours
SYNCBOARD_TOKEN_EXPIRE_MINUTES=1440

# OPTIONAL: Storage configuration (legacy file-based storage)
# NOTE: Phase 6 uses database storage, this is for migration only
SYNCBOARD_STORAGE_PATH=storage.json
SYNCBOARD_VECTOR_DIM=256

# OPTIONAL: Tesseract OCR path (Windows only)
# Uncomment and set path if using Windows:
# TESSERACT_CMD=C:\Program Files\Tesseract-OCR\tesseract.exe

# CELERY (Phase 2): Background task queue
# Redis connection URL for Celery message broker and result backend
# For Docker: redis://redis:6379/0
# For local dev: redis://localhost:6379/0
REDIS_URL=redis://localhost:6379/0
CELERY_BROKER_URL=redis://localhost:6379/0
CELERY_RESULT_BACKEND=redis://localhost:6379/0

# TRANSCRIPTION: Model to use for audio/video transcription
# Options: gpt-4o-mini-transcribe (recommended), whisper-1
TRANSCRIPTION_MODEL=gpt-4o-mini-transcribe

# AI MODELS: OpenAI models for different AI features
# GPT-5 models available: gpt-5, gpt-5-mini, gpt-5-nano
# All support 272k input / 128k output tokens
# Pricing: gpt-5 ($1.25/$10), gpt-5-mini ($0.25/$2), gpt-5-nano ($0.05/$0.40) per 1M tokens

# IDEA_MODEL: Model for generating project idea seeds
# Default: gpt-5-mini (good balance of quality and cost)
IDEA_MODEL=gpt-5-mini

# SUMMARY_MODEL: Model for hierarchical document summarization
# Default: gpt-5-nano (fast and cost-effective for summaries)
SUMMARY_MODEL=gpt-5-nano

# OPENAI_CONCEPT_MODEL: Model for AI concept extraction
# Default: gpt-5-nano (efficient for concept identification)
OPENAI_CONCEPT_MODEL=gpt-5-nano

# OPENAI_SUGGESTION_MODEL: Model for generating build suggestions
# Default: gpt-5-mini (better quality for complex suggestions)
OPENAI_SUGGESTION_MODEL=gpt-5-mini

# FLOWER: Celery monitoring dashboard
# Access at http://localhost:5555
# Default credentials (change in production!)
FLOWER_USER=admin
FLOWER_PASSWORD=admin

# =============================================================================
# OAuth Configuration for User Login
# =============================================================================
# Enable users to login with Google or GitHub accounts

# GOOGLE OAuth
# Get credentials from: https://console.cloud.google.com/apis/credentials
# Create OAuth 2.0 Client ID (Web Application)
GOOGLE_CLIENT_ID=your-google-client-id.apps.googleusercontent.com
GOOGLE_CLIENT_SECRET=your-google-client-secret
OAUTH_GOOGLE_REDIRECT_URI=http://localhost:8000/auth/google/callback

# GITHUB OAuth
# Get credentials from: https://github.com/settings/developers
# Create a new OAuth App
GITHUB_CLIENT_ID=your-github-client-id
GITHUB_CLIENT_SECRET=your-github-client-secret
OAUTH_GITHUB_REDIRECT_URI=http://localhost:8000/auth/github/callback

# Frontend URL for OAuth redirects
# Change to your production frontend URL
FRONTEND_URL=http://localhost:3000

# =============================================================================
# LLM Provider Configuration
# =============================================================================
# SyncBoard supports multiple LLM backends for AI features

# LLM_PROVIDER: Choose which LLM backend to use
# Options: "openai" (default), "ollama" (self-hosted), "mock" (testing)
LLM_PROVIDER=openai

# OLLAMA Configuration (for self-hosted LLM)
# Install: https://ollama.ai/download
# Models: ollama pull llama2, ollama pull codellama, ollama pull mistral
# Server: ollama serve (runs on port 11434)
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_CONCEPT_MODEL=llama2
OLLAMA_SUGGESTION_MODEL=llama2

# Recommended Ollama models by capability:
# - General: llama2, llama2:13b, mistral, mixtral
# - Coding: codellama, deepseek-coder, starcoder2
# - Fast: llama2:7b, phi2, tinyllama
# - Best quality: llama2:70b, mixtral:8x7b (requires GPU)
